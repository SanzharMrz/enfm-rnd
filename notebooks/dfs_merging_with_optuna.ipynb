{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3752056-5664-4cd9-a2fb-e9f386fc65d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/es_sanzhar/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/es_sanzhar/lib/python3.10/site-packages/cma/s.py:13: UserWarning: Could not import matplotlib.pyplot, therefore ``cma.plot()`` etc. is not available\n",
      "  _warnings.warn('Could not import matplotlib.pyplot, therefore'\n",
      "/opt/conda/envs/es_sanzhar/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import optuna\n",
    "from cma.evolution_strategy import CMAEvolutionStrategy\n",
    "import os \n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, GenerationConfig\n",
    "\n",
    "DEFAULT_RANDOM_SEED = 666\n",
    "\n",
    "def seed_everything(seed=DEFAULT_RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13443482-5cb6-4b56-b93d-ae3beb31340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Vikhrmodels/GrandMaster-PRO-MAX\", split='test').to_pandas()[['conversation']]\n",
    "dataset = dataset[dataset.conversation.apply(len) == 2]\n",
    "dataset_shuffled = dataset.sample(2300, random_state=DEFAULT_RANDOM_SEED).sample(frac=1, random_state=DEFAULT_RANDOM_SEED).reset_index(drop=True)\n",
    "train, val, test = dataset_shuffled.iloc[:1000], dataset_shuffled.iloc[1000:1300], dataset_shuffled.iloc[1300:2300]\n",
    "extract_content = lambda df: df.conversation.apply(lambda x: [x[0]['content'], x[1]['content']])\n",
    "train, val, test = map(extract_content, [train, val, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8727a361-52ea-4ae9-bb6e-a0ac43c7c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiply(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module to multiply input tensor by a scalar alpha.\n",
    "    \n",
    "    Args:\n",
    "        alpha (float): The scalar value to multiply the input tensor by.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: float):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to multiply input tensor by alpha.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Scaled tensor.\n",
    "        \"\"\"\n",
    "        return torch.mul(x, self.alpha)\n",
    "\n",
    "\n",
    "class EvolutionaryStacking:\n",
    "    \"\"\"\n",
    "    A class that performs evolutionary stacking using either CMA-ES or Optuna for optimization.\n",
    "\n",
    "    Args:\n",
    "        models_list (list): List of models to be stacked.\n",
    "        r (int): Number of repetitions.\n",
    "        M (int): Number of layers in each model.\n",
    "        n (int): Number of models.\n",
    "        method (str): Optimization method, either \"optuna\" or \"cma\". Default is \"optuna\".\n",
    "        population_size (int): Population size for CMA-ES. Default is 50.\n",
    "        num_generations (int): Number of generations for CMA-ES. Default is 100.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        models_list: list, \n",
    "        r: int, \n",
    "        M: int, \n",
    "        n: int, \n",
    "        method: str = \"optuna\", \n",
    "        population_size: int = 50, \n",
    "        num_generations: int = 100\n",
    "    ):\n",
    "        self.models_list = models_list\n",
    "        self.r = r\n",
    "        self.M = M\n",
    "        self.n = n\n",
    "        self.W_dim = M * r * n\n",
    "        self.method = method\n",
    "        self.population_size = population_size\n",
    "        self.num_generations = num_generations\n",
    "        \n",
    "        if method == \"cma\":\n",
    "            self.cma_es_I = CMAEvolutionStrategy([1] + [0] * (self.W_dim - 2) + [1], 0.1)\n",
    "            self.cma_es_W = CMAEvolutionStrategy([0.5] * (self.W_dim * (self.W_dim + 1) // 2), 0.2)\n",
    "\n",
    "    def forward_merged_model(self, merged_model: dict, input_ids: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the merged model to compute logits and loss.\n",
    "    \n",
    "        Args:\n",
    "            merged_model (dict): A dictionary containing the model components.\n",
    "            input_ids (torch.Tensor): Input tensor with token IDs.\n",
    "            labels (torch.Tensor): Target tensor with token IDs.\n",
    "    \n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss.\n",
    "        \"\"\"\n",
    "        # Pass through the embedding layer\n",
    "        emb = merged_model[\"embed\"](input_ids)\n",
    "    \n",
    "        # Generate rotary embeddings\n",
    "        position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        rot = merged_model[\"rotary\"](emb, position_ids)\n",
    "    \n",
    "        # Pass through the layers\n",
    "        out = emb\n",
    "        for idx, layer in enumerate(merged_model[\"layers\"]):\n",
    "            if idx % 2 == 0:\n",
    "                # Ensure the input is a tensor and not a tuple\n",
    "                if isinstance(out, tuple):\n",
    "                    out = out[0]  # Extract the tensor if it's a tuple\n",
    "                out = layer(out)  # Multiply layer\n",
    "            else:\n",
    "                out = layer(hidden_states=out, position_embeddings=rot)  # Transformer block\n",
    "    \n",
    "        # Pass through the final linear layer to get logits\n",
    "        logits = merged_model[\"lm_head\"](out)\n",
    "    \n",
    "        # Compute the loss\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    def calculate_perplexity(self, model, tokenizer, data: list[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model given a list of instructions and perfect responses.\n",
    "    \n",
    "        Args:\n",
    "            model: The pre-trained causal language model.\n",
    "            tokenizer: The tokenizer corresponding to the model.\n",
    "            instructions (list[str]): List of input instructions.\n",
    "            responses (list[str]): List of corresponding perfect responses.\n",
    "    \n",
    "        Returns:\n",
    "            float: The average perplexity across all instructions and responses.\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "    \n",
    "        for instruction, response in data:\n",
    "            input_text = instruction + response\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "            labels = inputs.input_ids.clone()\n",
    "            labels[:, :len(tokenizer(instruction).input_ids)] = -100\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                loss = self.forward_merged_model(model, inputs.input_ids, labels)\n",
    "                total_loss += loss.item() * (labels != -100).sum().item()\n",
    "                total_tokens += (labels != -100).sum().item()\n",
    "    \n",
    "        average_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "    \n",
    "        return perplexity\n",
    "\n",
    "    def W_to_matrix(self, W: np.ndarray, dim: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts a flat vector W into a matrix form.\n",
    "    \n",
    "        Args:\n",
    "            W (np.ndarray): Flattened weight matrix.\n",
    "            dim (int): Dimension of the square matrix.\n",
    "    \n",
    "        Returns:\n",
    "            np.ndarray: Reconstructed square matrix from W.\n",
    "    \n",
    "        Raises:\n",
    "            ValueError: If the length of W is not compatible with the expected size of the upper triangular matrix.\n",
    "        \"\"\"\n",
    "        expected_size = (dim * (dim - 1)) // 2\n",
    "        if len(W) != expected_size:\n",
    "            raise ValueError(f\"Length of W ({len(W)}) does not match the expected size for an upper triangular matrix of dimension {dim}x{dim}, which should be {expected_size}.\")\n",
    "    \n",
    "        W_matrix = np.zeros((dim, dim))\n",
    "        cnt = 0\n",
    "        for i in range(dim):\n",
    "            for j in range(dim - i - 1):\n",
    "                W_matrix[i][j] = W[cnt]\n",
    "                cnt += 1\n",
    "        return W_matrix\n",
    "\n",
    "\n",
    "    def get_ith_layer(self, model, i: int):\n",
    "        \"\"\"\n",
    "        Retrieves the ith layer from a model.\n",
    "\n",
    "        Args:\n",
    "            model: The model from which to retrieve the layer.\n",
    "            i (int): Index of the layer to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            The ith layer of the model.\n",
    "        \"\"\"\n",
    "        return list(model.model.layers.children())[i]\n",
    "\n",
    "    def construct_merged_model(self, I: list[int], W: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Constructs a merged model from the input models and the weight matrix.\n",
    "\n",
    "        Args:\n",
    "            I (list[int]): Binary list indicating the presence of layers.\n",
    "            W (np.ndarray): Weight matrix.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary representing the constructed merged model.\n",
    "        \"\"\"\n",
    "        model = {}\n",
    "        model[\"embed\"] = list(self.models_list[0].model.children())[0]\n",
    "        model[\"layers\"] = []\n",
    "        prev_idx = 0\n",
    "        for rep in range(self.r):\n",
    "            for n in range(len(self.models_list)):\n",
    "                for m in range(self.M):\n",
    "                    if I[m + n * self.M + rep * self.M * len(self.models_list)] == 1:\n",
    "                        model[\"layers\"].append(Multiply(W[prev_idx][m + n * self.M + rep * self.M * len(self.models_list)]))\n",
    "                        prev_idx = m + n * self.M + rep * self.M * len(self.models_list) + 1\n",
    "                        model[\"layers\"].append(self.get_ith_layer(self.models_list[n], m))\n",
    "        model[\"layers\"].append(list(self.models_list[0].model.children())[2])\n",
    "        model[\"rotary\"] = list(self.models_list[0].model.children())[3]\n",
    "        model[\"lm_head\"] = self.models_list[0].lm_head\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, I: list[int], W: np.ndarray, data) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates the model based on the given binary vector and weight matrix.\n",
    "\n",
    "        Args:\n",
    "            I (list[int]): Binary list indicating the presence of layers.\n",
    "            W (np.ndarray): Weight matrix.\n",
    "            data: Data to evaluate the model on.\n",
    "\n",
    "        Returns:\n",
    "            float: The calculated perplexity of the model.\n",
    "        \"\"\"\n",
    "        W_matrix = self.W_to_matrix(W, self.W_dim)\n",
    "        model = self.construct_merged_model(I, W_matrix)\n",
    "        return self.calculate_perplexity(model, tokenizer, data)\n",
    "\n",
    "    def optuna_objective(self, trial: optuna.trial.Trial, train_data) -> float:\n",
    "        \"\"\"\n",
    "        Objective function for Optuna optimization.\n",
    "    \n",
    "        Args:\n",
    "            trial (optuna.trial.Trial): A single trial of the Optuna study.\n",
    "            train_data: Data to train the model on.\n",
    "    \n",
    "        Returns:\n",
    "            float: The evaluation metric (perplexity) of the model.\n",
    "        \"\"\"\n",
    "        I_list = [trial.suggest_int(f\"I_{i}\", 0, 1) for i in range(self.W_dim)]\n",
    "        W_dim_upper_triangle = (self.W_dim * (self.W_dim - 1)) // 2\n",
    "        W_list = [trial.suggest_float(f\"W_{i}\", 0.0, 1.0) for i in range(W_dim_upper_triangle)]\n",
    "        return self.evaluate_model(I_list, W_list, train_data)\n",
    "\n",
    "    def run_cma_evolution(self, train_data, val_data):\n",
    "        \"\"\"\n",
    "        Runs the evolution using the CMA-ES method.\n",
    "\n",
    "        Args:\n",
    "            train_data: Training data.\n",
    "            val_data: Validation data.\n",
    "\n",
    "        Returns:\n",
    "            list: Best models found during the evolution.\n",
    "        \"\"\"\n",
    "        best_models = []\n",
    "        for n in range(self.num_generations):\n",
    "            I_list = self.cma_es_I.ask(number=self.population_size)\n",
    "            W_list = self.cma_es_W.ask(number=self.population_size)\n",
    "            metrics = [self.evaluate_model(I_list[i], W_list[i], train_data) for i in range(len(I_list))]\n",
    "            self.cma_es_I.tell(I_list, metrics)\n",
    "            self.cma_es_W.tell(W_list, metrics)\n",
    "            val_metrics = [self.evaluate_model(I_list[i], W_list[i], val_data) for i in range(len(I_list))]\n",
    "            best_idx = np.argmin(val_metrics)\n",
    "            best_models.append((I_list[best_idx], W_list[best_idx], metrics[best_idx]))\n",
    "            print(f\"Generation {n}, best val perplexity: {val_metrics[best_idx]}\")\n",
    "        return best_models\n",
    "\n",
    "    def run_optuna_optimization(self, train_data, val_data, n_trials: int):\n",
    "        \"\"\"\n",
    "        Runs the optimization using the Optuna method.\n",
    "\n",
    "        Args:\n",
    "            train_data: Training data.\n",
    "            val_data: Validation data.\n",
    "            n_trials (int): Number of trials for the Optuna study.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Best I, best W, and best validation perplexity.\n",
    "        \"\"\"\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(lambda trial: self.optuna_objective(trial, train_data), n_trials=n_trials)\n",
    "\n",
    "        best_trial = study.best_trial\n",
    "        best_I = [best_trial.params[f\"I_{i}\"] for i in range(self.W_dim)]\n",
    "        best_W = [best_trial.params[f\"W_{i}\"] for i in range(self.W_dim)]\n",
    "\n",
    "        val_perplexity = self.evaluate_model(best_I, best_W, val_data)\n",
    "        print(f\"Best validation perplexity: {val_perplexity}\")\n",
    "        \n",
    "        return best_I, best_W, val_perplexity\n",
    "\n",
    "    def run(self, train_data, val_data, n_trials: int = 100):\n",
    "        \"\"\"\n",
    "        Executes the selected optimization method.\n",
    "\n",
    "        Args:\n",
    "            train_data: Training data.\n",
    "            val_data: Validation data.\n",
    "            n_trials (int): Number of trials for Optuna (if using Optuna). Default is 100.\n",
    "\n",
    "        Returns:\n",
    "            The result of the selected optimization method.\n",
    "        \"\"\"\n",
    "        if self.method == \"cma\":\n",
    "            return self.run_cma_evolution(train_data, val_data)\n",
    "        elif self.method == \"optuna\":\n",
    "            return self.run_optuna_optimization(train_data, val_data, n_trials)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose either 'cma' or 'optuna'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba270ad6-af20-4407-823c-9ca409d9e3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:58<00:00, 44.65s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.65s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Vikhrmodels/it-5.4-fp16-orpo-v2'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cpu\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cpu\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd98c6b4-a5dc-4e6f-9a3f-54fe233a6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [model, model2]\n",
    "M = 32\n",
    "r = 1\n",
    "n = 2\n",
    "n_trials = 100\n",
    "method = \"optuna\"\n",
    "\n",
    "evolutionary_stacking = EvolutionaryStacking(models_list, r, M, n, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d731c2b-b48a-44e8-98db-f6f4cf4d70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-23 15:34:54,946] A new study created in memory with name: no-name-7a3654d2-bc35-4dc6-bd00-dedfd8d53951\n"
     ]
    }
   ],
   "source": [
    "best_models = evolutionary_stacking.run(train, val, n_trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_sanzhar",
   "language": "python",
   "name": "es_sanzhar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
